[![Maintenance](https://img.shields.io/badge/Maintained%3F-yes-green.svg)](https://GitHub.com/Naereen/StrapDown.js/graphs/commit-activity)
[![PR's Welcome](https://img.shields.io/badge/PRs-welcome-brightgreen.svg?style=flat)](http://makeapullrequest.com) 
[![Awesome](https://cdn.rawgit.com/sindresorhus/awesome/d7305f38d29fed78fa85652e3a63e154dd8e8829/media/badge.svg)](https://github.com/sindresorhus/awesome)

# <p align=center>`Awesome CV Foundational Models`</p>

A curated list of awesome papers in computer vision. This paper supplements our survey paper. We intend to continuously update it. 

We strongly encourage authors of relevant works to make a pull request and add their paper's information. 

**Authors**: [Muhammad Awais](awaisrauf.github.io), [Muzammal Naseer](https://muzammal-naseer.netlify.app), [Salman Khan](https://salman-h-khan.github.io), [Rao Muhammad Anwer](https://scholar.google.fi/citations?user=_KlvMVoAAAAJ&hl=en), [Hisham Cholakkal](https://scholar.google.com/citations?user=bZ3YBRcAAAAJ&hl=en), [Mubarak Shah](https://www.crcv.ucf.edu/person/mubarak-shah/), [Fahad Shahbaz Khan](https://sites.google.com/view/fahadkhans/home) 

<div align='center'>
<img src="overview.svg" width="60%" height="60%">
</div>



## Textually Prompted Models

**Scaling up visual and vision-language representation learning with noisy text supervision**  2021 <br> Jia, Chao,  Yang, Yinfei,  Xia, Ye,  Chen, Yi-Ting,  Parekh, Zarana,  Pham, Hieu,  Le, Quoc,  Sung, Yun-Hsuan,  Li, Zhen,  Duerig, Tom <br>  [[**Link**]]() 
 
**WenLan: Bridging vision and language by large-scale multi-modal pre-training**  2021 <br> Huo, Yuqi,  Zhang, Manli,  Liu, Guangzhen,  Lu, Haoyu,  Gao, Yizhao,  Yang, Guoxing,  Wen, Jingyuan,  Zhang, Heng,  Xu, Baogui,  Zheng, Weihao,  others <br>  [[**Link**]]() 
 
**Florence: A new foundation model for computer vision**  2021 <br> Yuan, Lu,  Chen, Dongdong,  Chen, Yi-Ling,  Codella, Noel,  Dai, Xiyang,  Gao, Jianfeng,  Hu, Houdong,  Huang, Xuedong,  Li, Boxin,  Li, Chunyuan,  others <br>  [[**Link**]]() 
 
**FILIP: fine-grained interactive language-image pre-training**  2021 <br> Yao, Lewei,  Huang, Runhui,  Hou, Lu,  Lu, Guansong,  Niu, Minzhe,  Xu, Hang,  Liang, Xiaodan,  Li, Zhenguo,  Jiang, Xin,  Xu, Chunjing <br>  [[**Link**]]() 
 
**Slip: Self-supervision meets language-image pre-training**  2022 <br> Mu, Norman,  Kirillov, Alexander,  Wagner, David,  Xie, Saining <br>  [[**Link**]]( https://github.com/facebookresearch/SLIP ) 
 
**Scaling language-image pre-training via masking**  2023 <br> Li, Yanghao,  Fan, Haoqi,  Hu, Ronghang,  Feichtenhofer, Christoph,  He, Kaiming <br>  [[**Link**]]( https://github.com/facebookresearch/flip ) 
 
**Maskclip: Masked self-distillation advances contrastive language-image pretraining**  2023 <br> Dong, Xiaoyi,  Bao, Jianmin,  Zheng, Yinglin,  Zhang, Ting,  Chen, Dongdong,  Yang, Hao,  Zeng, Ming,  Zhang, Weiming,  Yuan, Lu,  Chen, Dong,  others <br>  [[**Link**]](  https://github.com/LightDXY/MaskCLIP ) 
 
**An Inverse Scaling Law for CLIP Training**  2023 <br> Xianhang Li,  Zeyu Wang,  Cihang Xie <br>  [[**Link**]]() 
 
**CLIPA-v2: Scaling CLIP Training with 81.1% Zero-shot ImageNet Accuracy within a \$10,000 Budget; An Extra \$4,000 Unlocks 81.8% Accuracy**  2023 <br> Xianhang Li,  Zeyu Wang,  Cihang Xie <br>  [[**Link**]]() 
 
**Eva: Exploring the limits of masked visual representation learning at scale**  2023 <br> Fang, Yuxin,  Wang, Wen,  Xie, Binhui,  Sun, Quan,  Wu, Ledell,  Wang, Xinggang,  Huang, Tiejun,  Wang, Xinlong,  Cao, Yue <br>  [[**Link**]](  https://github.com/baaivision/EVA ) 
 
**Eva-clip: Improved training techniques for clip at scale**  2023 <br> Sun, Quan,  Fang, Yuxin,  Wu, Ledell,  Wang, Xinlong,  Cao, Yue <br>  [[**Link**]]() 
 
**Eva-02: A visual representation for neon genesis**  2023 <br> Fang, Yuxin,  Sun, Quan,  Wang, Xinggang,  Huang, Tiejun,  Wang, Xinlong,  Cao, Yue <br>  [[**Link**]]() 
 
**Reproducible scaling laws for contrastive language-image learning**  2023 <br> Cherti, Mehdi,  Beaumont, Romain,  Wightman, Ross,  Wortsman, Mitchell,  Ilharco, Gabriel,  Gordon, Cade,  Schuhmann, Christoph,  Schmidt, Ludwig,  Jitsev, Jenia <br>  [[**Link**]]( https://github.com/mlfoundations/open_clip ) 
 
**Extract Free Dense Labels from CLIP**  2022 <br> Zhou, Chong,  Loy, Chen Change,  Dai, Bo <br>  [[**Link**]]( https://github.com/chongzhou96/MaskCLIP ) 
 
**Grounded language-image pre-training**  2022 <br> Li, Liunian Harold,  Zhang, Pengchuan,  Zhang, Haotian,  Yang, Jianwei,  Li, Chunyuan,  Zhong, Yiwu,  Wang, Lijuan,  Yuan, Lu,  Zhang, Lei,  Hwang, Jenq-Neng,  others <br>  [[**Link**]]( https://github.com/DerrickWang005/CRIS.pytorch) 
 
**Grounding dino: Marrying dino with grounded pre-training for open-set object detection**  
 <br> Liu, Shilong,  Zeng, Zhaoyang,  Ren, Tianhe,  Li, Feng,  Zhang, Hao,  Yang, Jie,  Li, Chunyuan,  Yang, Jianwei,  Su, Hang,  Zhu, Jun,  others <br>  [[**Link**]]() 
 
**Simple Open-Vocabulary Object Detection with Vision Transformers**  2022 <br> Matthias Minderer,  Alexey Gritsenko,  Austin Stone,  Maxim Neumann,  Dirk Weissenborn,  Alexey Dosovitskiy,  Aravindh Mahendran,  Anurag Arnab,  Mostafa Dehghani,  Zhuoran Shen,  Xiao Wang,  Xiaohua Zhai,  Thomas Kipf,  Neil Houlsby <br>  [[**Link**]]() 
 
**Open-vocabulary object detection via vision and language knowledge distillation**  2021 <br> Gu, Xiuye,  Lin, Tsung-Yi,  Kuo, Weicheng,  Cui, Yin <br>  [[**Link**]](https://github.com/tensorflow/tpu/tree/master/models/official/detection/projects/vild ) 
 
**GroupViT: Semantic Segmentation Emerges from Text Supervision**  2022 <br> Jiarui Xu,  Shalini De Mello,  Sifei Liu,  Wonmin Byeon,  Thomas Breuel,  J. Kautz,  X. Wang <br>  [[**Link**]](  https://github.com/NVlabs/GroupViT ) 
 
**Scaling Open-Vocabulary Image Segmentation with Image-Level Labels**  2021 <br> Golnaz Ghiasi,  Xiuye Gu,  Yin Cui,  Tsung-Yi Lin <br>  [[**Link**]]( https://github.com/tensorflow/tpu/tree/641c1ac6e26ed788327b973582cbfa297d7d31e7/models/official/detection/projects/openseg ) 
 
**Multimodal few-shot learning with frozen language models**  2021 <br> Tsimpoukelli, Maria,  Menick, Jacob L,  Cabi, Serkan,  Eslami, SM,  Vinyals, Oriol,  Hill, Felix <br>  [[**Link**]]() 
 
**Flamingo: a visual language model for few-shot learning**  2022 <br> Alayrac, Jean-Baptiste,  Donahue, Jeff,  Luc, Pauline,  Miech, Antoine,  Barr, Iain,  Hasson, Yana,  Lenc, Karel,  Mensch, Arthur,  Millican, Katherine,  Reynolds, Malcolm,  others <br>  [[**Link**]]() 
 
**Language Models are General-Purpose Interfaces**  2022 <br> Yaru Hao,  Haoyu Song,  Li Dong,  Shaohan Huang,  Zewen Chi,  Wenhui Wang,  Shuming Ma,  Furu Wei <br>  [[**Link**]]() 
 
**Language Is Not All You Need: Aligning Perception with Language Models**  2023 <br> Shaohan Huang,  Li Dong,  Wenhui Wang,  Yaru Hao,  Saksham Singhal,  Shuming Ma,  Tengchao Lv,  Lei Cui,  Owais Khan Mohammed,  Barun Patra,  Qiang Liu,  Kriti Aggarwal,  Zewen Chi,  Johan Bjorck,  Vishrav Chaudhary,  Subhojit Som,  Xia Song,  Furu Wei <br>  [[**Link**]]() 
 
**Simvlm: Simple visual language model pretraining with weak supervision**  2021 <br> Wang, Zirui,  Yu, Jiahui,  Yu, Adams Wei,  Dai, Zihang,  Tsvetkov, Yulia,  Cao, Yuan <br>  [[**Link**]]() 
 
**mPLUG-Owl: Modularization Empowers Large Language Models with Multimodality**  2023 <br> Qinghao Ye,  Haiyang Xu,  Guohai Xu,  Jiabo Ye,  Ming Yan,  Yiyang Zhou,  Junyang Wang,  Anwen Hu,  Pengcheng Shi,  Yaya Shi,  Chaoya Jiang,  Chenliang Li,  Yuanhong Xu,  Hehong Chen,  Junfeng Tian,  Qian Qi,  Ji Zhang,  Fei Huang <br>  [[**Link**]]() 
 
**Image Captioners Are Scalable Vision Learners Too**  2023 <br> Tschannen, Michael,  Kumar, Manoj,  Steiner, Andreas,  Zhai, Xiaohua,  Houlsby, Neil,  Beyer, Lucas <br>  [[**Link**]]() 
 
**Uniter: Universal image-text representation learning**  2020 <br> Chen, Yen-Chun,  Li, Linjie,  Yu, Licheng,  El Kholy, Ahmed,  Ahmed, Faisal,  Gan, Zhe,  Cheng, Yu,  Liu, Jingjing <br>  [[**Link**]]( https://github.com/ChenRocks/UNITER ) 
 
**A unified sequence interface for vision tasks**  2022 <br> Chen, Ting,  Saxena, Saurabh,  Li, Lala,  Lin, Tsung-Yi,  Fleet, David J,  Hinton, Geoffrey E <br>  [[**Link**]]() 
 
**Unifying vision-and-language tasks via text generation**  2021 <br> Cho, Jaemin,  Lei, Jie,  Tan, Hao,  Bansal, Mohit <br>  [[**Link**]](  ) 
 
**Coca: Contrastive captioners are image-text foundation models**  2022 <br> Yu, Jiahui,  Wang, Zirui,  Vasudevan, Vijay,  Yeung, Legg,  Seyedhosseini, Mojtaba,  Wu, Yonghui <br>  [[**Link**]]() 
 
**Flava: A foundational language and vision alignment model**  2022 <br> Singh, Amanpreet,  Hu, Ronghang,  Goswami, Vedanuj,  Couairon, Guillaume,  Galuba, Wojciech,  Rohrbach, Marcus,  Kiela, Douwe <br>  [[**Link**]]( https://github.com/facebookresearch/multimodal/tree/main/examples/flava) 
 
**Pali: A jointly-scaled multilingual language-image model**  2022 <br> Chen, Xi,  Wang, Xiao,  Changpinyo, Soravit,  Piergiovanni, AJ,  Padlewski, Piotr,  Salz, Daniel,  Goodman, Sebastian,  Grycner, Adam,  Mustafa, Basil,  Beyer, Lucas,  others <br>  [[**Link**]]() 
 
**Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation**  2022 <br> Li, Junnan,  Li, Dongxu,  Xiong, Caiming,  Hoi, Steven <br>  [[**Link**]]( https://github.com/salesforce/BLIP) 
 
**Bridge-Tower: Building Bridges Between Encoders in Vision-Language Representation Learning**  2022 <br> Xu, Xiao,  Wu, Chenfei,  Rosenman, Shachar,  Lal, Vasudev,  Duan, Nan <br>  [[**Link**]](  
https://github.com/microsoft/BridgeTower ) 
 
**Toward Building General Foundation Models for Language, Vision, and Vision-Language Understanding Tasks**  2023 <br> Zhang, Xinsong,  Zeng, Yan,  Zhang, Jipeng,  Li, Hang <br>  [[**Link**]]() 
 
**Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models**  2023 <br> Li, Junnan,  Li, Dongxu,  Savarese, Silvio,  Hoi, Steven <br>  [[**Link**]]( 
https://github.com/salesforce/LAVIS/tree/main/projects/blip2) 
 
**InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning**  2023 <br> Wenliang Dai,  Junnan Li,  Dongxu Li,  Anthony Meng Huat Tiong,  Junqi Zhao,  Weisheng Wang,  Boyang Li,  Pascale Fung,  Steven Hoi <br>  [[**Link**]]( https://github.com/salesforce/LAVIS/tree/main/projects/instructblip ) 
 
**TaCA: Upgrading Your Visual Foundation Model with Task-agnostic Compatible Adapter**  2023 <br> Binjie Zhang,  Yixiao Ge,  Xuyuan Xu,  Ying Shan,  Mike Zheng Shou <br>  [[**Link**]](  https://github.com/TencentARC/TaCA ) 
 
**Transfer Visual Prompt Generator across LLMs**  2023 <br> Ao Zhang,  Hao Fei,  Yuan Yao,  Wei Ji,  Li Li,  Zhiyuan Liu,  Tat-Seng Chua <br>  [[**Link**]](    https://github.com/VPGTrans/VPGTrans ) 
 
**Coarse-to-fine vision-language pre-training with fusion in the backbone**  2022 <br> Dou, Zi-Yi,  Kamath, Aishwarya,  Gan, Zhe,  Zhang, Pengchuan,  Wang, Jianfeng,  Li, Linjie,  Liu, Zicheng,  Liu, Ce,  LeCun, Yann,  Peng, Nanyun,  others <br>  [[**Link**]](   https://github.com/microsoft/FIBER ) 
 
**Detecting Everything in the Open World: Towards Universal Object Detection**  2023 <br> Wang, Zhenyu,  Li, Yali,  Chen, Xi,  Lim, Ser-Nam,  Torralba, Antonio,  Zhao, Hengshuang,  Wang, Shengjin <br>  [[**Link**]]( https://github.com/zhenyuw16/UniDetector ) 
 
**Generalized Decoding for Pixel, Image and Language**  2022 <br> Zou, Xueyan,  Dou, Zi-Yi,  Yang, Jianwei,  Gan, Zhe,  Li, Linjie,  Li, Chunyuan,  Dai, Xiyang,  Wang, Jianfeng,  Yuan, Lu,  Peng, Nanyun,  Wang, Lijuan,  Lee, Yong Jae,  Gao, Jianfeng <br>  [[**Link**]]() 
 
**Glipv2: Unifying localization and vision-language understanding**  2022 <br> Zhang, Haotian,  Zhang, Pengchuan,  Hu, Xiaowei,  Chen, Yen-Chun,  Li, Liunian,  Dai, Xiyang,  Wang, Lijuan,  Yuan, Lu,  Hwang, Jenq-Neng,  Gao, Jianfeng <br>  [[**Link**]]( https://github.com/microsoft/GLIP) 
 


## Visually Prompted Models

**Seggpt: Segmenting everything in context**  2023 <br> Wang, Xinlong,  Zhang, Xiaosong,  Cao, Yue,  Wang, Wen,  Shen, Chunhua,  Huang, Tiejun <br>  [[**Link**]]( https://github.com/baaivision/Painter/tree/main/SegGPT ) 
 
**Segment anything**  2023 <br> Kirillov, Alexander,  Mintun, Eric,  Ravi, Nikhila,  Mao, Hanzi,  Rolland, Chloe,  Gustafson, Laura,  Xiao, Tete,  Whitehead, Spencer,  Berg, Alexander C,  Lo, Wan-Yen,  others <br>  [[**Link**]]() 
 
**Segment everything everywhere all at once**  2023 <br> Zou, Xueyan,  Yang, Jianwei,  Zhang, Hao,  Li, Feng,  Li, Linjie,  Gao, Jianfeng,  Lee, Yong Jae <br>  [[**Link**]]() 
 
**Caption Anything: Interactive Image Description with Diverse Multimodal Controls**  2023 <br> Teng Wang,  Jinrui Zhang,  Junjie Fei,  Hao Zheng,  Yunlong Tang,  Zhe Li,  Mingqi Gao,  Shanshan Zhao <br>  [[**Link**]]() 
 
**Track Anything: Segment Anything Meets Videos**  2023 <br> Jinyu Yang,  Mingqi Gao,  Zhe Li,  Shang Gao,  Fangjing Wang,  Feng Zheng <br>  [[**Link**]]() 
 
**Segment and track anything**  2023 <br> Cheng, Yangming,  Li, Liulei,  Xu, Yuanyou,  Li, Xiaodi,  Yang, Zongxin,  Wang, Wenguan,  Yang, Yi <br>  [[**Link**]]() 
 
**Segment Anything Meets Point Tracking**  2023 <br> Rajič, Frano,  Ke, Lei,  Tai, Yu-Wing,  Tang, Chi-Keung,  Danelljan, Martin,  Yu, Fisher <br>  [[**Link**]]() 
 
**SAM-DA: UAV Tracks Anything at Night with SAM-Powered Domain Adaptation
**  2023 <br> Liangliang, Yao,  Haobo, Zuo,  Guangze, Zheng,  Changhong, Fu,  Jia, Pan <br>  [[**Link**]]() 
 
**RSPrompter: Learning to Prompt for Remote Sensing Instance Segmentation based on Visual Foundation Model**  2023 <br> Keyan Chen,  Chenyang Liu,  Hao Chen,  Haotian Zhang,  Wenyuan Li,  Zhengxia Zou,  Zhenwei Shi <br>  [[**Link**]]( https://github.com/KyanChen/RSPrompter ) 
 
**Segment Anything in Medical Images**  2023 <br> Jun Ma,  Bo Wang <br>  [[**Link**]]() 
 
**AutoSAM: Adapting SAM to Medical Images by Overloading the Prompt Encoder**  2023 <br> Shaharabany, Tal,  Dahan, Aviad,  Giryes, Raja,  Wolf, Lior <br>  [[**Link**]]() 
 
**3DSAM-adapter: Holistic Adaptation of SAM from 2D to 3D for Promptable Medical Image Segmentation**  2023 <br> Shizhan Gong,  Yuan Zhong,  Wenao Ma,  Jinpeng Li,  Zhao Wang,  Jingyang Zhang,  Pheng-Ann Heng,  Qi Dou <br>  [[**Link**]]() 
 
**DeSAM: Decoupling Segment Anything Model for Generalizable Medical Image Segmentation**  2023 <br> Gao, Yifan,  Xia, Wei,  Hu, Dingdu,  Gao, Xin <br>  [[**Link**]]( https://github.com/yifangao112/DeSAM ) 
 
**MedLSAM: Localize and Segment Anything Model for 3D Medical Images**  2023 <br> Wenhui, Lei,  Xu, Wei,  Xiaofan, Zhang,  Kang, Li,  Shaoting Zhang <br>  [[**Link**]]( https://github.com/openmedlab/MedLSAM ) 
 
**Faster Segment Anything: Towards Lightweight SAM for Mobile Applications**  2023 <br> Chaoning Zhang,  Dongshen Han,  Yu Qiao,  Jung Uk Kim,  Sung-Ho Bae,  Seungkyu Lee,  Choong Seon Hong <br>  [[**Link**]]( https://github.com/ChaoningZhang/MobileSAM ) 
 
**Fast Segment Anything**  2023 <br> Xu Zhao,  Wenchao Ding,  Yongqi An,  Yinglong Du,  Tao Yu,  Min Li,  Ming Tang,  Jinqiao Wang <br>  [[**Link**]]() 
 
**RefSAM: Efficiently Adapting Segmenting Anything Model for Referring Video Object Segmentation**  2023 <br> Yonglin, Li,  Jing, Zhang,  Xiao, Teng,  Long, Lan <br>  [[**Link**]]( https://github.com/LancasterLi/RefSAM) 
 
**Images speak in images: A generalist painter for in-context visual learning**  2023 <br> Wang, Xinlong,  Wang, Wen,  Cao, Yue,  Shen, Chunhua,  Huang, Tiejun <br>  [[**Link**]]( https://github.com/baaivision/Painter/tree/main/Painter ) 
 
**VisionLLM: Large Language Model is also an Open-Ended Decoder for Vision-Centric Tasks**  2023 <br> Wenhai Wang,  Zhe Chen,  Xiaokang Chen,  Jiannan Wu,  Xizhou Zhu,  Gang Zeng,  Ping Luo,  Tong Lu,  Jie Zhou,  Yu Qiao,  Jifeng Dai <br>  [[**Link**]]( https://github.com/OpenGVLab/VisionLLM ) 
 
**Prismer: A Vision-Language Model with An Ensemble of Experts**  2023 <br> Shikun Liu,  Linxi Fan,  Edward Johns,  Zhiding Yu,  Chaowei Xiao,  Anima Anandkumar <br>  [[**Link**]]( https://github.com/NVlabs/prismer ) 
 
**CLIP2Video: Mastering Video-Text Retrieval via Image CLIP**  2021 <br> Han Fang,  Pengfei Xiong,  Luhui Xu,  Yu Chen <br>  [[**Link**]]( https://github.com/CryhanFang/CLIP2Video ) 
 
**AudioCLIP: Extending CLIP to Image, Text and Audio**  2021 <br> Andrey Guzhov,  Federico Raue,  Jörn Hees,  Andreas Dengel <br>  [[**Link**]]( https://github.com/AndreyGuzhov/AudioCLIP ) 
 
**ImageBind: One Embedding Space To Bind Them All**  2023 <br> Rohit Girdhar,  Alaaeldin El-Nouby,  Zhuang Liu,  Mannat Singh,  Kalyan Vasudev Alwala,  Armand Joulin,  Ishan Misra <br>  [[**Link**]]( https://github.com/facebookresearch/ImageBind ) 
 
**Macaw-LLM: Multi-Modal Language Modeling with Image, Video, Audio, and Text Integration**  2023 <br> Chenyang Lyu,  Bingshuai Liu,  Minghao Wu,  Zefeng Du,  Xinting Huang,  Zhaopeng Tu,  Shuming Shi,  Longyue Wang <br>  [[**Link**]]() 
 
**COSA: Concatenated Sample Pretrained Vision-Language Foundation Model**  2023 <br> Sihan Chen,  Xingjian He,  Handong Li,  Xiaojie Jin,  Jiashi Feng,  Jing Liu <br>  [[**Link**]]() 
 
**Valley: Video Assistant with Large Language model Enhanced abilitY**  2023 <br> Ruipu Luo,  Ziwang Zhao,  Min Yang,  Junwei Dong,  Minghui Qiu,  Pengcheng Lu,  Tao Wang,  Zhongyu Wei <br>  [[**Link**]]( https://github.com/RupertLuo/Valley ) 
 
**PaLM-E: An Embodied Multimodal Language Model**  2023 <br> Danny Driess,  Fei Xia,  Mehdi S. M. Sajjadi,  Corey Lynch,  Aakanksha Chowdhery,  Brian Ichter,  Ayzaan Wahid,  Jonathan Tompson,  Quan Vuong,  Tianhe Yu,  Wenlong Huang,  Yevgen Chebotar,  Pierre Sermanet,  Daniel Duckworth,  Sergey Levine,  Vincent Vanhoucke,  Karol Hausman,  Marc Toussaint,  Klaus Greff,  Andy Zeng,  Igor Mordatch,  Pete Florence <br>  [[**Link**]]() 
 
**VIMA: General Robot Manipulation with Multimodal Prompts**  2022 <br> Yunfan Jiang,  Agrim Gupta,  Zichen Zhang,  Guanzhi Wang,  Yongqiang Dou,  Yanjun Chen,  Li Fei-Fei,  Anima Anandkumar,  Yuke Zhu,  Linxi Fan <br>  [[**Link**]]( https://github.com/vimalabs/VIMA ) 
 
**MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge**  2022 <br> Linxi Fan,  Guanzhi Wang,  Yunfan Jiang,  Ajay Mandlekar,  Yuncong Yang,  Haoyi Zhu,  Andrew Tang,  De-An Huang,  Yuke Zhu,  Anima Anandkumar <br>  [[**Link**]]( https://github.com/MineDojo/MineDojo ) 
 
**Voyager: An open-ended embodied agent with large language models**  2023 <br> Wang, Guanzhi,  Xie, Yuqi,  Jiang, Yunfan,  Mandlekar, Ajay,  Xiao, Chaowei,  Zhu, Yuke,  Fan, Linxi,  Anandkumar, Anima <br>  [[**Link**]]( https://github.com/MineDojo/Voyager ) 
 
**LM-Nav: Robotic Navigation with Large Pre-Trained Models of Language, Vision, and Action**  2022 <br> Dhruv Shah,  Blazej Osinski,  Brian Ichter,  Sergey Levine <br>  [[**Link**]]( https://github.com/blazejosinski/lm_nav ) 
 
